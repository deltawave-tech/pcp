# NaN gradients from `stablehlo.power` (exponent) — root cause and fix

This note documents the NaN issue we hit during NanoChat → PCP training parity, why it happened, and what was changed to fix it.

## What failed (symptom)

The first red flag showed up in the PCP ↔ PyTorch gradient parity check:

- Test: `test/nanochat/test_pcp_gradients_parity.py`
- Symptom: one or more parameter gradients coming back from PCP contained `NaN` while PyTorch’s reference gradient did not.
- Typical output looked like: `param_0_grad produced NaNs (actual_nan=True expected_nan=False)`

This meant the forward pass could still be correct (and it was), but *the backward pass produced invalid values*.

## Where the NaNs came from (root cause)

PCP’s training VMFB includes a gradient function generated by the Zig autodiff engine:

- Autodiff engine: `src/autodiff/engine.zig`
- VJP rules: `src/autodiff/vjp_rules.zig`

The NaNs were caused by PCP’s VJP rule for:

- `stablehlo.power`

### The old rule (problematic)

The classic derivative identity is:

```
y = base^exp
dy/dbase = exp * base^(exp - 1)
```

The previous VJP implementation built `base^(exp-1)` by literally creating:

- `exp_minus_1 = exp - 1`
- `pow(base, exp_minus_1)`

This is mathematically fine, but it introduced a *compiler/backend lowering problem* when `exp_minus_1` is **not a compile-time constant**.

Even if `exp` is a constant tensor in the forward graph, doing `exp - 1` inside the generated gradient function can keep it “dynamic” (not folded), and some backends lower:

```
pow(x, y)  ->  exp(y * log(x))
```

That lowering is only valid over the reals when `x > 0`. If `x` is negative, `log(x)` becomes NaN.

### Minimal “noob” example of the bug

Assume the forward op is a square:

- `base = -2.0`
- `exp = 2.0`
- `y = pow(-2, 2) = 4`

The true derivative is:

- `dy/dbase = 2 * (-2)^(1) = -4`

But the *generated gradient graph* computed:

- `exp_minus_1 = exp - 1` (often not folded to a compile-time constant)
- `pow(base, exp_minus_1)` (a `stablehlo.power` with a dynamic exponent)

If the backend lowers that dynamic `pow` as `exp((exp_minus_1) * log(base))`, it tries:

- `log(-2.0)` → `NaN`

So the gradient becomes `NaN`, even though `(-2)^1` is perfectly valid and should equal `-2`.

That is why the NaN wasn’t “random” or “machine dependent” in the usual sense: it was a deterministic domain error introduced by a particular lowering strategy for `pow` with a non-constant exponent.

## The fix (what changed)

### New `stablehlo.power` VJP rule

We rewrote the base-gradient formula to avoid building `base^(exp-1)` as another `pow`:

```
y = base^exp
base^(exp-1) = y / base           (when base != 0)
dy/dbase = grad_out * exp * (y / base)
```

This keeps the computation in well-behaved ops (`multiply`, `divide`, `select`) and avoids the problematic `pow(base, dynamic_exponent)` entirely.

**Zero guard**

`y/base` is undefined at `base == 0` (it becomes `0/0` for `exp > 0`), which would create NaNs again.
So we explicitly define:

- `y/base = 0` when `base == 0`

This is correct for the main case we care about in NanoChat (squares / norms): if `exp=2`, then `dy/dbase = 2*base`, and at `base=0` the derivative is `0`.

**Code reference**

- Fix implementation: `src/autodiff/vjp_rules.zig` (function `powerVJP`)

### Extra constant detection (supporting fix)

`isValueFromConstantOp` was updated to actually detect constant results:

- now recognizes `stablehlo.constant` and `arith.constant`

This improves a few VJP rules that early-out for constant operands (e.g. embedding gradients when the operand is constant).

**Code reference**

- `src/autodiff/vjp_rules.zig` (`isValueFromConstantOp`)

## Why other files changed (not just `vjp_rules.zig`)

The NaN fix itself is in `src/autodiff/vjp_rules.zig`, but the parity test harness required a couple of “plumbing” fixes to make the suite reliable and debuggable:

1) **Prevent test deadlocks from gigantic logs**
   - `src/network/tcp_stream.zig` used to log the *full JSON payload* (which can include big base64 blobs of params/state).
   - In subprocess-based tests, stdout pipes can fill up → the worker blocks while “logging”, never sending the message the shepherd is waiting for.
   - Fix: keep the small preview log by default and only log the full payload when an explicit verbose flag is enabled.

2) **Make verbose debug logs optional (not default)**
   - Some debug prints are extremely noisy and slow.
   - We reintroduced the removed debug logs, but gated them behind a build option:
     - build flag: `-Dpcp_verbose_logs=true`
   - Implementation:
     - `build.zig` now emits an options module `build_options` with `pcp_verbose_logs`
     - Zig files import `build_options` and guard their debug prints.

3) **Docs**
   - `test/tests.md` documents:
     - how to enable verbose PCP logs
     - how to work around the common `libstdc++.so.6` import error for Torch in this Nix environment

## How we verified the fix

After the change:

- `test/nanochat/test_pcp_gradients_parity.py` passes:
  - no NaNs in gradients
  - gradient diffs are within tolerance after applying the same clipping rules
- The forward-loss parity test still passes:
  - `test/nanochat/test_generate_nanochat_mlir_forward_loss.py`

## Remaining caveats / future work

- The exponent gradient is still treated as zero in `powerVJP`. That’s intentional for now (we only need `d/d(base)` in current graphs).
- The `base==0` branch is chosen to match the square/norm use case. If we start using `pow` with other exponents where `x=0` is problematic (e.g. negative exponents), we should revisit this.
- You may see `error(gpa): ... leaked` warnings in Debug builds during export. Those are allocator leak reports and are not the same as NaN math errors; they should be fixed separately.

## Quick “where to look” index

- NaN fix: `src/autodiff/vjp_rules.zig` → `powerVJP`
- Gradient function builder: `src/autodiff/engine.zig`
- Verbose logging flag: `build.zig` (`-Dpcp_verbose_logs=true`)
- TCP payload logging gate: `src/network/tcp_stream.zig`
