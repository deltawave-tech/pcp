#!/bin/bash

# PCP Distributed Training System - End-to-End Test Script
# This script demonstrates the complete MLIR â†’ SPIR-V â†’ Metal pipeline
# across distributed worker nodes on your M3 Mac Pro

echo "ğŸŒ‘ PCP Distributed Training System - Live Test"
echo "=============================================="

# Build the distributed training system
echo "â›“ï¸ Forging the distributed computation engine..."
zig build

if [ $? -ne 0 ]; then
    echo "ğŸ’£ Build failed. The forge is cold. Address compilation errors before proceeding."
    exit 1
fi

echo "ğŸŒ™ Binary synthesis complete: ./zig-out/bin/main_distributed"
echo "---"

# Verify the executable exists
if [ ! -f "./zig-out/bin/main_distributed" ]; then
    echo "ğŸ’£ Executable not materialized. Build system anomaly detected."
    exit 1
fi

# Kill any previous instances on exit
trap "echo 'ğŸŒ‘ Terminating all distributed processes...'; kill 0" EXIT

echo "ğŸ‰ Initiating distributed tensor computation orchestration..."
echo ""

# 1. Start the Shepherd Coordinator in the background
echo "ğŸ‘¹ Materializing Shepherd Coordinator..."
./zig-out/bin/main_distributed --shepherd --workers 2 &
SHEPHERD_PID=$!
echo "   â””â”€ Shepherd Process ID: $SHEPHERD_PID"
sleep 3 # Allow Shepherd to establish TCP listener

# 2. Start Worker Alpha in the background
echo "ğŸ‰ Connecting Worker Alpha to the network..."
./zig-out/bin/main_distributed --worker --connect 127.0.0.1:8080 &
WORKER1_PID=$!
echo "   â””â”€ Worker Alpha Process ID: $WORKER1_PID"
sleep 2

# 3. Start Worker Beta in the background
echo "ğŸ‰ Connecting Worker Beta to the network..."
./zig-out/bin/main_distributed --worker --connect 127.0.0.1:8080 &
WORKER2_PID=$!
echo "   â””â”€ Worker Beta Process ID: $WORKER2_PID"
sleep 1

echo ""
echo "â›“ï¸ Distributed System Architecture Active"
echo "========================================="
echo "Shepherd Coordinator: PID $SHEPHERD_PID (127.0.0.1:8080)"
echo "Worker Alpha:         PID $WORKER1_PID"
echo "Worker Beta:          PID $WORKER2_PID"
echo ""
echo "ğŸŒ‘ Expected Execution Flow:"
echo "   â”œâ”€ Shepherd constructs complete MLIR computation graphs"
echo "   â”œâ”€ Workers receive serialized graphs via TCP"
echo "   â”œâ”€ Workers parse MLIR modules and execute on Metal"
echo "   â”œâ”€ Real SPIR-V generation and GPU shader compilation"
echo "   â”œâ”€ Distributed parameter updates via DiLoCo algorithm"
echo "   â””â”€ Complete end-to-end tensor computation pipeline"
echo ""
echo "ğŸŒ™ Critical Success Patterns to Observe:"
echo "   âœ“ 'DiLoCo: Building worker training graph...'"
echo "   âœ“ 'Serialized MLIR module to X bytes'"
echo "   âœ“ 'Training graph + parameters broadcasted to 2 workers'"
echo "   âœ“ 'Worker X received training graph.'"
echo "   âœ“ 'Deserialized MLIR module from X bytes'"
echo "   âœ“ 'Real SPIR-V binary size: X bytes (stub was 20 bytes)'"
echo "   âœ“ 'Successfully executed MLIR module on Metal hardware'"
echo "   âœ“ 'Worker X completed inner loop and sent results.'"
echo ""
echo "â›“ï¸ Monitor the cascade of MLIR compilation across distributed nodes..."
echo "   Press Ctrl+C to terminate the distributed system."
echo ""

# Wait for the shepherd process to complete training (or for Ctrl+C)
wait $SHEPHERD_PID

echo ""
echo "ğŸŒ‘ Distributed training orchestration complete."
echo "   The tensor computation pipeline has demonstrated end-to-end execution"
echo "   across symbolic MLIR graphs, SPIR-V compilation, and Metal hardware."